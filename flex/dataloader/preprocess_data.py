import json
import os
import shutil
import sys

import cv2
import imageio
import numpy as np
import tifffile as tiff
import torch
from lietorch import SE3
from PIL import Image
from rectification import StereoRectifier

sys.path.append(os.getcwd())
import argparse

import imageio.v2 as iio
from torchvision import transforms as T
from tqdm.auto import tqdm, trange

from flex.render.util.util import draw_poses
from local.eval_poses import get_cam2world
from local.util import read_freiburg, read_poses


def generate_gif(file_paths):
    transform = T.ToTensor()
    all_images = []
    for file_path in file_paths:
        img = Image.open(os.path.join(basedir + "/images", file_path))

        img = transform(img)  # (3, h, w)
        img = img.permute(1, 2, 0)  # (h*w, 3) RGB
        img = (img * 255).numpy().astype(np.uint8)

        all_images += [img]

    tqdm.write("DEMO|Generate rendering gif...")
    imageio.mimwrite(
        f"{basedir}/video.mp4",
        np.stack(np.array(all_images[::2])),
        fps=35,
        format="FFMPEG",
        quality=8,
    )
    fps = 35
    if len(all_images) > 2000:
        fps = (len(all_images) / 2000) * fps
    with iio.get_writer(f"{basedir}/video.gif", mode="I", duration=1 / fps) as writer:
        for frame in all_images:
            writer.append_data(frame)


def adjust_depth(poses, scale, basedir, dict=dict):
    # fix depth images (scale depth images like poses):
    if scale != 1.0:
        os.makedirs(os.path.join(basedir, "depth_adj"), exist_ok=True)
        depth_file_paths_original = sorted(
            os.listdir(os.path.join(basedir, "depth_original"))
        )
        max_depth = 0
        # find max depth value for normalization
        for j in range(len(poses)):
            depths = np.array(
                Image.open(
                    os.path.join(basedir, "depth_original/")
                    + depth_file_paths_original[j]
                ),
                dtype=np.float64,
            )
            if np.max(depths) > max_depth:
                max_depth = np.max(depths)

        # rescale depth and save!
        new_max = 0
        for j in range(len(depth_file_paths_original)):
            depths = np.array(
                Image.open(
                    os.path.join(basedir, "depth_original/")
                    + depth_file_paths_original[j]
                ),
                dtype=np.float64,
            )

            depths /= scale

            # rescale poses translation accordingly
            if np.max(depths) > new_max:
                new_max = np.max(depths)
            tiff.imsave(
                os.path.join(basedir, "depth_adj/")
                + depth_file_paths_original[j][:-3]
                + "tiff",
                depths,
            )

        print("Old Max. Depth:" + str(max_depth))
        print("New Max. Depth: " + str(new_max))
        print(poses[:10, :, 3])
        print(f"Poses Trans. Max: {np.max(poses[:,:,3])}")
        print(f"Poses Trans. Min: {np.min(poses[:,:,3])}")

    return poses


def create_transforms(
    i_train,
    poses,
    basedir,
    tool_mask,
    flow_data,
    file_paths,
    depth_file_paths,
    tool_mask_paths,
    fwd_paths,
    bwd_paths,
    all_frames,
    i_test,
    split="train",
    dict=dict,
):
    frames = []
    for i in i_train:
        item = {}
        item["file_path"] = "images/" + file_paths[i][:-4]
        item["transform_matrix"] = poses[i, :, :4].tolist()
        item["transform_matrix"].append([0, 0, 0, 1])
        item["time"] = float(i) / (len(file_paths) - 1)  # correct version

        item["depth_file_path"] = basedir + "/depth_adj/" + depth_file_paths[i]

        if tool_mask:
            item["tool_mask_path"] = basedir + "/masks/" + tool_mask_paths[i]

        if flow_data:
            if i < (len(all_frames) - 1):
                item["fwd_file_path"] = basedir + "/flow_ds/fwd/" + fwd_paths[i + 1]
            else:
                item["fwd_file_path"] = basedir + "/flow_ds/fwd/" + fwd_paths[0]
            item["bwd_file_path"] = basedir + "/flow_ds/bwd/" + bwd_paths[i]

            item["fwd_mask"] = 1 if i < (len(all_frames) - 1) else 0
            item["bwd_mask"] = 1 if i != 0 else 0
            item["prev_test"] = 1 if (i - 1 in i_test) else 0
            item["post_test"] = 1 if (i + 1 in i_test) else 0

        frames.append(item)

    dict["frames"] = frames

    with open(os.path.join(basedir, "transforms_" + split + ".json"), "w") as outfile:
        json.dump(dict, outfile)


def create_hexd(
    scene="23",
    factor=1,
    flow_data=False,
    data_type="stereomis",
    stereomis_gt=False,
    tool_mask=True,
    own_poses=False,
    generate_gif=False,
    calib_img_size=None,
):
    if data_type == "miti":
        # basedir = '/media/storage/stilz/HexPlane/data/miti_slam_nerf/' + str(scene)
        basedir = "data/miti_slam_nerf/" + str(scene)
    elif data_type == "stereomis":
        basedir = "data/StereoMIS/" + str(scene)
    else:
        AssertionError("Unknown Data Type!")

    # load poses
    if data_type == "stereomis":
        if stereomis_gt:
            poses_arr = read_poses(pose_path=os.path.join(basedir, "groundtruth.txt"))
        else:
            poses_arr = read_poses(
                pose_path=os.path.join(basedir, "trajectory.freiburg")
            )
            # poses_arr = poses_arr[1:] # fix duplicate of first entry
        poses = poses_arr

    else:
        poses_arr = np.load(os.path.join(basedir, "poses_bounds.npy"))

    length = len(poses_arr)

    # Select the data split.
    i_test = np.arange(length)[::8]
    i_train = np.array([i for i in np.arange(int(length)) if i not in i_test])

    all_frames = np.arange(length)

    if not (data_type == "stereomis"):
        file_paths = sorted(os.listdir(os.path.join(basedir, "images")))
    else:
        file_paths = sorted(os.listdir(os.path.join(basedir, "video_frames")))
        # use only left images
        keep_files = [item for item in file_paths if "l" in item]
        if len(keep_files) != 0:
            file_paths = keep_files
            # move files to new folder
            os.makedirs(os.path.join(basedir, "images"), exist_ok=True)
            current_root = os.path.join(basedir, "video_frames")
            for file_path in file_paths:
                shutil.move(
                    os.path.join(current_root, file_path),
                    os.path.join(basedir, "images"),
                )
        else:
            file_paths = sorted(os.listdir(os.path.join(basedir, "images")))

        if generate_gif:
            generate_gif(file_paths)

    if flow_data:
        fwd_paths = sorted(os.listdir(os.path.join(basedir, "flow_ds/fwd")))
        bwd_paths = sorted(os.listdir(os.path.join(basedir, "flow_ds/bwd")))
    if tool_mask:
        tool_mask_paths = sorted(os.listdir(os.path.join(basedir, "masks")))

    dict = {}

    # Set camera intrinsics
    dim = np.array(Image.open(os.path.join(basedir, "images/") + file_paths[0])).shape
    dict["w"], dict["h"] = dim[1], dim[0]
    print(dict["w"], dict["h"])
    dict["camera_model"] = "OPENCV"

    if data_type == "miti":
        dict["fl_x"], dict["fl_y"], dict["cx"], dict["cy"] = (
            1.5709233828878441e03 / 2,
            1.5709233828878441e03 / 2,
            8.0491165252677206e02 / 2,
            5.2790839350304282e02 / 2,
        )
    elif data_type == "stereomis":
        rect_mode = "conventional"
        img_size = [dict["w"], dict["h"]]
        if calib_img_size is not None:
            img_size = calib_img_size

        calib_file = os.path.join(basedir, "StereoCalibration.ini")
        rect = StereoRectifier(calib_file, img_size_new=img_size, mode=rect_mode)
        calib = rect.get_rectified_calib()
        print(calib)

        dict["fl_x"], dict["fl_y"] = (
            calib["intrinsics"]["left"][0, 0],
            calib["intrinsics"]["left"][1, 1],
        )
        dict["cx"] = calib["intrinsics"]["left"][0, 2]
        dict["cy"] = calib["intrinsics"]["left"][1, 2]

    if data_type == "stereomis" or data_type == "miti":
        # Convert from OpenCV to OpenGL Format
        print(f"Poses Trans. Max. Before: {np.max(poses[:,:,3])}")
        print(f"Poses Trans. Min. Before: {np.min(poses[:,:,3])}")
        init_max_pose_t = np.max(poses[:, :, 3])
        flip = np.eye(4)
        flip[1, 1] = -1
        flip[2, 2] = -1
        poses = np.matmul(poses, flip)

        if data_type != "stereomis":
            poses = poses[:, :3, :4]

        if data_type == "stereomis":
            bds = np.array([0.01, 12.0]).reshape(1, 2)
            bds = np.repeat(bds, len(poses), axis=0)
            hwf = np.array([dict["h"], dict["w"], dict["fl_x"]]).reshape(1, 3, 1)
            hwf = np.repeat(hwf, len(poses), axis=0)

            scale = 1.0
            poses, scale, center_shift = center_poses_0(poses)
            print(f"ReScale: {scale}")
            print(f"ReCenter: {center_shift}")

            poses_3x5 = np.concatenate((poses[:, :3, :4], hwf), axis=-1)
            poses_flat = poses_3x5.reshape(-1, 15)
            poses_bounds = np.concatenate((poses_flat, bds), axis=-1)

            np.save(basedir + "/poses_bounds.npy", poses_bounds)
            print("saved poses bounds!")

            near_fars = poses_arr[:, -2:]
            poses = poses_bounds[:, :-2].reshape(-1, 3, 5).transpose([1, 2, 0])
            bds = poses_bounds[:, -2:].transpose([1, 0])
            bds = np.moveaxis(bds, -1, 0).astype(np.float32)

            poses = np.moveaxis(poses, -1, 0).astype(np.float32)

            if own_poses:
                poses_rot = torch.load(
                    os.path.join(basedir, "own_poses/poses_rot.th"),
                    map_location=torch.device("cpu"),
                ).detach()
                poses_t = torch.load(
                    os.path.join(basedir, "own_poses/poses_t.th"),
                    map_location=torch.device("cpu"),
                ).detach()
                poses = get_cam2world(poses_rot, poses_t).numpy()
                # 0-center poses!
                poses, new_scale, new_center_shift = center_poses_0(poses)
                scale *= new_scale
                center_shift += new_center_shift

    print(f"Poses Shape: {poses.shape}")
    dict["depth_scale_factor"] = scale.item()
    dict["recenter_trans"] = center_shift.tolist()
    poses = adjust_depth(poses, scale, basedir, dict=dict)
    depth_file_paths = sorted(os.listdir(os.path.join(basedir, "depth_adj")))

    # create dict for each frame:
    create_transforms(
        i_train,
        poses,
        basedir,
        tool_mask,
        flow_data,
        file_paths,
        depth_file_paths,
        tool_mask_paths,
        fwd_paths,
        bwd_paths,
        all_frames,
        i_test,
        split="train",
        dict=dict,
    )
    create_transforms(
        i_test,
        poses,
        basedir,
        tool_mask,
        flow_data,
        file_paths,
        depth_file_paths,
        tool_mask_paths,
        fwd_paths,
        bwd_paths,
        all_frames,
        i_test,
        split="test",
        dict=dict,
    )
    create_transforms(
        all_frames,
        poses,
        basedir,
        tool_mask,
        flow_data,
        file_paths,
        depth_file_paths,
        tool_mask_paths,
        fwd_paths,
        bwd_paths,
        all_frames,
        i_test,
        split="",
        dict=dict,
    )


def center_poses_0(poses):
    center = poses[..., 3].mean(0)
    poses[..., 3] = poses[..., 3] - center
    max_poses = abs(poses[..., 3]).max(0)
    scale = max_poses.max(0)
    print(f"Max poses along translation axis: {max_poses} and scale value: {scale}")
    poses[..., 3] = (poses[..., 3]) / scale

    return poses, scale, center


def center_poses(poses, blender2opencv):
    """
    Center the poses so that we can use NDC.
    See https://github.com/bmild/nerf/issues/34
    Inputs:
        poses: (N_images, 3, 4)
    Outputs:
        poses_centered: (N_images, 3, 4) the centered poses
        pose_avg: (3, 4) the average pose
    """
    poses = poses @ blender2opencv
    pose_avg = average_poses(poses)  # (3, 4)
    pose_avg_homo = np.eye(4)
    pose_avg_homo[:3] = (
        pose_avg  # convert to homogeneous coordinate for faster computation
    )
    pose_avg_homo = pose_avg_homo
    # by simply adding 0, 0, 0, 1 as the last row
    last_row = np.tile(np.array([0, 0, 0, 1]), (len(poses), 1, 1))  # (N_images, 1, 4)
    poses_homo = np.concatenate(
        [poses, last_row], 1
    )  # (N_images, 4, 4) homogeneous coordinate

    poses_centered = np.linalg.inv(pose_avg_homo) @ poses_homo  # (N_images, 4, 4)
    #     poses_centered = poses_centered  @ blender2opencv
    poses_centered = poses_centered[:, :3]  # (N_images, 3, 4)

    return poses_centered, pose_avg_homo


def average_poses(poses):
    """
    Calculate the average pose, which is then used to center all poses
    using @center_poses. Its computation is as follows:
    1. Compute the center: the average of pose centers.
    2. Compute the z axis: the normalized average z axis.
    3. Compute axis y': the average y axis.
    4. Compute x' = y' cross product z, then normalize it as the x axis.
    5. Compute the y axis: z cross product x.

    Note that at step 3, we cannot directly use y' as y axis since it's
    not necessarily orthogonal to z axis. We need to pass from x to y.
    Inputs:
        poses: (N_images, 3, 4)
    Outputs:
        pose_avg: (3, 4) the average pose
    """
    # 1. Compute the center
    center = poses[..., 3].mean(0)  # (3)

    # 2. Compute the z axis
    z = normalize(poses[..., 2].mean(0))  # (3)

    # 3. Compute axis y' (no need to normalize as it's not the final output)
    y_ = poses[..., 1].mean(0)  # (3)

    # 4. Compute the x axis
    x = normalize(np.cross(z, y_))  # (3)

    # 5. Compute the y axis (as z and x are normalized, y is already of norm 1)
    y = np.cross(x, z)  # (3)

    pose_avg = np.stack([x, y, z, center], 1)  # (3, 4)

    return pose_avg


def recenter_poses(poses):
    poses_ = poses + 0
    bottom = np.reshape([0, 0, 0, 1.0], [1, 4])
    c2w = poses_avg(poses)
    c2w = np.concatenate([c2w[:3, :4], bottom], -2)
    bottom = np.tile(np.reshape(bottom, [1, 1, 4]), [poses.shape[0], 1, 1])
    poses = np.concatenate([poses[:, :3, :4], bottom], -2)

    poses = np.linalg.inv(c2w) @ poses
    poses_[:, :3, :4] = poses[:, :3, :4]
    poses = poses_
    return poses


def normalize(x):
    return x / np.linalg.norm(x)


def viewmatrix(z, up, pos):
    vec2 = normalize(z)
    vec1_avg = up
    vec0 = normalize(np.cross(vec1_avg, vec2))
    vec1 = normalize(np.cross(vec2, vec0))
    m = np.stack([vec0, vec1, vec2, pos], 1)
    return m


def ptstocam(pts, c2w):
    tt = np.matmul(c2w[:3, :3].T, (pts - c2w[:3, 3])[..., np.newaxis])[..., 0]
    return tt


def poses_avg(poses):
    hwf = poses[0, :3, -1:]

    center = poses[:, :3, 3].mean(0)
    vec2 = normalize(poses[:, :3, 2].sum(0))
    up = poses[:, :3, 1].sum(0)
    c2w = np.concatenate([viewmatrix(vec2, up, center), hwf], 1)

    return c2w


def _generate_spherical_poses(poses, bds):
    """Generate a 360 degree spherical path for rendering."""
    # pylint: disable=g-long-lambda
    p34_to_44 = lambda p: np.concatenate(
        [p, np.tile(np.reshape(np.eye(4)[-1, :], [1, 1, 4]), [p.shape[0], 1, 1])], 1
    )
    rays_d = poses[:, :3, 2:3]
    rays_o = poses[:, :3, 3:4]

    def min_line_dist(rays_o, rays_d):
        a_i = np.eye(3) - rays_d * np.transpose(rays_d, [0, 2, 1])
        b_i = -a_i @ rays_o
        pt_mindist = np.squeeze(
            -np.linalg.inv((np.transpose(a_i, [0, 2, 1]) @ a_i).mean(0)) @ (b_i).mean(0)
        )
        return pt_mindist

    pt_mindist = min_line_dist(rays_o, rays_d)
    center = pt_mindist
    up = (poses[:, :3, 3] - center).mean(0)
    vec0 = normalize(up)
    vec1 = normalize(np.cross([0.1, 0.2, 0.3], vec0))
    vec2 = normalize(np.cross(vec0, vec1))
    pos = center
    c2w = np.stack([vec1, vec2, vec0, pos], 1)
    poses_reset = np.linalg.inv(p34_to_44(c2w[None])) @ p34_to_44(poses[:, :3, :4])
    rad = np.sqrt(np.mean(np.sum(np.square(poses_reset[:, :3, 3]), -1)))
    sc = 1.0 / rad
    poses_reset[:, :3, 3] *= sc
    bds *= sc
    rad *= sc
    centroid = np.mean(poses_reset[:, :3, 3], 0)
    zh = centroid[2]
    radcircle = np.sqrt(rad**2 - zh**2)
    new_poses = []

    for th in np.linspace(0.0, 2.0 * np.pi, 120):
        camorigin = np.array([radcircle * np.cos(th), radcircle * np.sin(th), zh])
        up = np.array([0, 0, -1.0])
        vec2 = normalize(camorigin)
        vec0 = normalize(np.cross(vec2, up))
        vec1 = normalize(np.cross(vec2, vec0))
        pos = camorigin
        p = np.stack([vec0, vec1, vec2, pos], 1)
        new_poses.append(p)

    new_poses = np.stack(new_poses, 0)
    new_poses = np.concatenate(
        [new_poses, np.broadcast_to(poses[0, :3, -1:], new_poses[:, :3, -1:].shape)], -1
    )
    poses_reset = np.concatenate(
        [
            poses_reset[:, :3, :4],
            np.broadcast_to(poses[0, :3, -1:], poses_reset[:, :3, -1:].shape),
        ],
        -1,
    )
    return poses_reset


if __name__ == "__main__":
    print("Start!")
    # console arguments
    parser = argparse.ArgumentParser()
    parser.add_argument("--scene", type=str, help="Scene name")
    parser.add_argument("--data_type", type=str, help="Dataset name")
    parser.add_argument(
        "--factor", type=int, default=1, help="Size ratio to initial images"
    )
    parser.add_argument("--flow_data", action="store_true")
    parser.add_argument("--tool_mask", action="store_true")
    parser.add_argument("--stereomis_gt", action="store_true")
    parser.add_argument("--own_poses", action="store_true")
    parser.add_argument("--generate_gif", action="store_true")
    parser.add_argument(
        "--calib_img_size",
        nargs="+",
        type=int,
        default=None,
        help="set image dimensions, epsecially needed when changing image via cropping etc. to fix calibration file",
    )
    args = parser.parse_args()
    create_hexd(
        scene=args.scene,
        factor=args.factor,
        flow_data=args.flow_data,
        data_type=args.data_type,
        stereomis_gt=args.stereomis_gt,
        tool_mask=args.tool_mask,
        own_poses=args.own_poses,
        generate_gif=args.generate_gif,
        calib_img_size=args.calib_img_size,
    )
    print("Done!")

    # Example
    # python flex/dataloader/preprocess_data.py --scene 'P2_8_1' --data_type 'stereomis' --flow_data --tool_mask
